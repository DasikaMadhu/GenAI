{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6790265-57f1-4a10-ba4c-de4361211759",
   "metadata": {},
   "source": [
    "# GenAI Essentials\n",
    "\n",
    "1. [NLP](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#NLP)\n",
    "2. [Regression](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Regression)\n",
    "3. [Classification](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Classification)\n",
    "4. [Clustering](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Clustering)\n",
    "5. [Association](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Association)\n",
    "6. [Dimensionality Reduction](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Dimensionality-Reduction)\n",
    "7. [Types of Machine Learning](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Types-of-Machine-Learning)\n",
    "8. [Classical ML](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Classical-Machine-Learning)\n",
    "9. [Differences among SL, UL & RL](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Differences-between-Supervised-Learning,-Unsupervised-Learning-&-Reinforcement-Learning)\n",
    "10. [Neural Networks and Deep Learning](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#Neural-Networks-and-Deep-Learning)\n",
    "11. [BERT](http://localhost:8888/lab/tree/Desktop/genAI/gen-essentials.ipynb?#BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f08bb-36b3-409d-894b-d2ff51463744",
   "metadata": {},
   "source": [
    "### NLP \n",
    "\n",
    "1. Text Wrangling & Preprocessing\n",
    "    1. Conversion - Converting uppercase to lowercase, contractions to full forms, etc\n",
    "    2. Sanitization - Removing HTML code or stop words or special characters\n",
    "    3. Tokenization - Converting text into vector embeddings\n",
    "    4. Stemming - Reducing words to their base or root form by cuting off prefixes and suffixes, i.e., affixes\n",
    "    5. Lemmatization - Reducing words to their base or dictionary form\n",
    "2. Language Understanding (Syntax & Structure)\n",
    "    1. Parts of Speech (POS) Tagging - Giving each word a particular part of speech (adverb, adjective, verb, etc.) or grammatical category\n",
    "    2. Chunking - Identify parts of speech and short phrases present in a given sentence\n",
    "    3. Dependency Parsing - Examining the dependencies between the phrases of a sentence in order to determine its grammatical structure\n",
    "    4. Constituency Parsing - Identifying the constituents, or subparts, of a sentence and the relationships between them, represented in a hierarchical structure\n",
    "4. Processing & Functionality\n",
    "    1. Named Entity Recognition (NER) - Identifying and classifying named entities in unstructured text, like people, organizations, locations, dates, and other relevant information\n",
    "    2. N-gram Identification - Analyzing text by breaking it into sequences of N consecutive words or characters to capture context and patterns in language\n",
    "    3. Sentiment Analysis - Determining whether a piece of text expresses a positive, negative, or neutral sentiment\n",
    "    4. Information Extraction - Extracting structured information (e.g., entities, relationships, or facts) from unstructured text\n",
    "    5. Information Retrieval - Searching and retrieving relevant information or documents from a large dataset or database based on a user query\n",
    "    6. Question Answering - Providing precise answers to user queries by understanding context and retrieving relevant information\n",
    "    7. Topic Modelling - Identifying underlying topics in a collection of documents by grouping related words and concepts\n",
    "\n",
    "### Regression \n",
    "\n",
    "* *Regression* is a statistical and machine learning technique used to model and analyze relationships between dependent and independent variables.\n",
    "* A *regression line* is the straight line that best fits the data points in a scatter plot, representing the predicted values of the dependent variable.\n",
    "* *Error* refers to the difference between the predicted and actual values, often minimized to improve the model.\n",
    "* *Distance* in regression measures how far a prediction is from the actual data point, often represented by metrics like Euclidean distance.\n",
    "* Regression represents input features as *vectors* in multidimensional space, where the model learns a function to map these vectors to the target.\n",
    "* Common regression algorithms include Simple Linear Regression, Multiple Linear Regression, Polynomial Regression, Lasso Regression, Ridge Regression, Decision Tree Regression, Random Forest Regression, and Support Vector Regression (SVR).\n",
    "* Examples - Predicting house prices (linear regression), stock prices (time-series regression), and customer lifetime value (logistic regression for binary cases).\n",
    "\n",
    "### Classification \n",
    "\n",
    "* *Classification* is a machine learning task where the goal is to assign a label or category to a given input based on its features.\n",
    "* A *classification line* (or decision boundary) is a boundary that separates different classes in a feature space, typically used in algorithms like logistic regression or support vector machines.\n",
    "* A *category* refers to the distinct labels or classes that data points can be assigned to in a classification problem, such as \"spam\" vs. \"not spam.\"\n",
    "* Common classification algorithms include Logistic Regression, Decision Trees, Random Forest, Support Vector Machine (SVM), Kernel SVM, K-Nearest Neighbors (KNN), Naive Bayes, XGBoost, and AdaBoost.\n",
    "* Examples - Email spam detection, image classification (e.g., cat vs. dog), and medical diagnosis (e.g., disease or no disease).\n",
    "\n",
    "### Clustering \n",
    "\n",
    "* *Clustering* is an unsupervised machine learning technique that groups similar data points together based on shared features, without predefined labels.\n",
    "* Clustering is useful for exploring data patterns, discovering hidden structures, and reducing dimensionality, often applied in anomaly detection and recommendation systems.\n",
    "* Common clustering algorithms include K-means, K-medoids, DBSCAN, and hierarchical clustering.\n",
    "* Examples - Customer segmentation in marketing, grouping news articles by topics, and organizing images based on visual similarity.\n",
    "\n",
    "### Association\n",
    "\n",
    "* *Association* is a rule-based machine learning technique used to discover interesting relationships, patterns, or dependencies among items in large datasets.\n",
    "* An association rule represents an \"if-then\" relationship, such as \"if a customer buys bread, they are likely to buy butter\".\n",
    "* Support refers to the proportion of transactions containing a specific item or itemset in the dataset.\n",
    "* Confidence measures how often the rule has been found to be true in the dataset, given the antecedent is present.\n",
    "* Lift indicates the strength of a rule by comparing its confidence to the expected confidence if the items were independent.\n",
    "* Common association algorithms include Apriori, Eclat, and FP-Growth.\n",
    "* Examples - Market basket analysis (e.g., \"people who buy diapers also buy beer\"), cross-selling strategies, and website clickstream analysis.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "* *Dimensionality Reduction* is a technique used to reduce the number of features or variables in a dataset while preserving as much relevant information as possible.\n",
    "* Often used as a pre-processing stage. \n",
    "* Feature selection techniques, such as filter, wrapper, and embedded methods, identify the most relevant features in the data.\n",
    "* Dimensionality reduction helps address issues like the \"curse of dimensionality\" and improves computational efficiency for high-dimensional data.\n",
    "    * Common algorithms include Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Generalized Discriminant Analysis (GDA), Singular Value Decomposition (SVD), Latent Dirichlet Allocation (LDA), and t-SNE.\n",
    "* Examples - Reducing features in gene expression datasets, image compression, and improving clustering or classification on high-dimensional data.\n",
    "\n",
    "### Types of Machine Learning \n",
    "\n",
    "##### Learning Problems\n",
    "1. Supervised - Training a model on labeled data, where the model learns to map inputs to known outputs\n",
    "2. Unsupervised - Training a model on unlabeled data, where the model identifies patterns and structures in the data without predefined labels\n",
    "3. Reinforcement - Training an agent to make decisions by interacting with an environment and receiving rewards or penalties based on actions\n",
    "\n",
    "##### Hybrid Learning Problems\n",
    "1. Semi-supervised - Combining very limited labeled and large number of unlabeled data to train a model, leveraging the labeled data for better performance\n",
    "2. Self-supervised - Generating labels from the input data itself, using parts of the data to predict other parts, where the model is pre-trained on unlabeled data by automatically generating labels that are further used as ground truths in subsequent iterations\n",
    "3. Multi-instance - Training a model with labeled bags of instances, where the label is assigned to the entire bag, not individual instances\n",
    "\n",
    "##### Statistical Inference \n",
    "1. Inductive - Drawing general conclusions from specific observations, often used to build models that generalize well to new data\n",
    "2. Deductive - Using general rules or premises to make specific predictions or conclusions, often relying on established theories or frameworks\n",
    "3. Transductive - Making predictions for specific test instances based on observed training data, without generalizing to all possible cases\n",
    "\n",
    "##### Learning Techniques \n",
    "1. Multi-task - Training a model on one dataset to perform multiple related tasks simultaneously, sharing common representations to improve overall performance\n",
    "2. Active - Interactively queries a user to select the most informative data points from unlabeled data for labeling, enabling it to achieve higher accuracy with fewer labeled examples\n",
    "3. Online - Training a model incrementally as new data becomes available, allowing it to adapt continuously over time, before a prediction is required or after the last observation\n",
    "4. Transfer - Leveraging knowledge from a pre-trained model on one task to improve performance on a related task\n",
    "5. Ensemble - Combining multiple models to improve predictions, often by averaging their outputs or using a voting mechanism to reduce overfitting and increase accuracy\n",
    "\n",
    "### Division of Machine Learning\n",
    "\n",
    "1. Classical ML - Simple data, clear features, cost efficient\n",
    "   1. Supervised\n",
    "   2. Unsupervised\n",
    "2. Reinforcement Learning - When there is no data, the model will figure out what to do through trial and error\n",
    "   1. Real-time decision making\n",
    "   2. Game AI\n",
    "   3. Learning Tasks\n",
    "   4. Robot Navigation \n",
    "3. Ensemble Methods - When the quality of the data is a problem\n",
    "   1. Bagging\n",
    "   2. Boosting\n",
    "   3. Tagging\n",
    "4. Deep Learning / Neural Networks - When the data is complicated and/or features are unclear\n",
    "   1. Convolutional Neural Networks (CNN)\n",
    "   2. Recurrent Neural Networks (RNN)\n",
    "   3. Generative Adversarial Networks (GAN)\n",
    "   4. Multilayer Perceptron (MLP)\n",
    "\n",
    "### Classical Machine Learning\n",
    "\n",
    "#### Supervised Learning (SL)\n",
    "* Data that has been labelled into categories\n",
    "* Task-driven\n",
    "* Goal is to make a prediction\n",
    "\n",
    "Classification - Predict what cetegory does the variables belong to (Identity Fraud Detection)\n",
    "Regression - Predict the target variable in the future (Market Forecast)\n",
    "\n",
    "#### Unsupervised Learning (UL)\n",
    "* Data is not labelled\n",
    "* Data-driven\n",
    "* Goal is to recognise a structure or pattern\n",
    "\n",
    "Clustering - Group data based on their similarities or differences (Targeted Marketing)\n",
    "Association - Find a relationship between variables through association (Customer Recommendation)\n",
    "Dimensionality Reduction - Helps in reducing the amount of data pre-processing (Big Data Visualization)\n",
    "\n",
    "\n",
    "##### Differences between Supervised Learning, Unsupervised Learning & Reinforcement Learning\n",
    "| Supervised Learning | Unsupervised Learning | Reinforcement Learning |\n",
    "|------------------|------------------|------------------|\n",
    "| An ML task or function that needs training data to predict the outcome. | An ML task or function trains on unlabelled training data to predict the outcome. | There is no data, an ML model generates data in an attempt to reach a goal. |\n",
    "| The ML model learns from labelled training data. | The ML model uses unlabelled data to to identify patterns or structures and applies its own labels. | The ML model is deployed within an environment and works on a decision-driven trial and error principle. |\n",
    "| More accurate than UL | Requires human intervention in validating the predicted outcomes | Dynamic and adapts over time to maximize long-term rewards |\n",
    "| Classification, Regression | Clustering, Association, Dimensionality Reduction | Real-time decision making, Game AI, Learning Tasks, Robot Navigation |\n",
    "\n",
    "\n",
    "### Neural Networks and Deep Learning\n",
    "\n",
    "#### Neural Networks \n",
    "* Mimics the brain.\n",
    "* A neuron / node represents an algorithm.\n",
    "* Data input to a neuron and based on the ouput, the data will be passed to other connected neurals.\n",
    "* The connection between neurons is weighted.\n",
    "* The network is organized in layers - an input layer, multiple hidden layers and an output layer.\n",
    "\n",
    "<img src=\"./gen_ai_1.png\" alt=\"Neural Network Architecture\" title=\"Neural Network Architecture\" width=\"500\"/>\n",
    "\r\n",
    "#### Deep Learning \n",
    "* A neural network that has 3 or more hidden layers.\n",
    "* *Feed Forward neural network* (FNN) - Connections between nodes do not form a cycle and always move forward\n",
    "* *Back Propogation* (BP) - Moving backward, to learn, through the neural network adjusting weights to improve the outcome on the next iteration\n",
    "* *Ground truth* - Labelled data that is known to be correct\n",
    "* *Error rate* - How bad the network performed\n",
    "* *Loss Function* - A function that compares the ground truth to the prediction to determine the error rate\n",
    "* *Activation Functions* - An algorithm applied to a hidden layer node that affects the connected ouput\n",
    "* *Dense* - The next layer increases the number of nodes\n",
    "* *Sparse* - The next layer decreases the amount of nodes\n",
    "\n",
    "##### Deep Learning Algorithms \n",
    "1. Supervised\n",
    "    * Fully-connected Feed Forward Networks (FNN)\n",
    "    * Recurrent Neural Networks (RNN)\n",
    "    * Convolutional Neural Networks (CNN)\n",
    "2. Unsupervised\n",
    "    * Deep Belief Networks (DBN)\n",
    "    * Stacked Auto Encoders (SAE)\n",
    "    * Restricted Boltzmann Machines (RBMs\n",
    "  ### [BERT](https://huggingface.co/docs/transformers/model_doc/bert)\r\n",
    "\r\n",
    "-**Bidirectional Encoder Representatios** (BERT) from Transformers.\r\n",
    "- Encoder-only models.- **Misner** (mainly non-directional) - It takes in all of the data at once (does not read right to left or left to right).\r\n",
    "- Reads the text from right to left and left to right to understand the context, and the sequence of words matters.\r\n",
    "- Taking the transformer architecture, stack the encoders to get BERT.\r\n",
    "- It highlights certain words based on their association with other words (nuances - when one word enhances another).\r\n",
    "- It assigns priority to words based on the understanding it has of language.\r\n",
    "- It is contextually aware of each word and its importance in a sentence.\r\n",
    "- The parameter size is constant before training, and their weights are initialized randomly.\r\n",
    "- Estimates how off it is and readjusts the weights accordingly until the error margin is minimized.\r\n",
    "- Post-training, the parameters have a representation of the data it trned on.\r\n",
    "- **Ptrained **:\r\n",
    "    1. **Masked LanguaModel (MLM)**  \r\n",
    "        - Provide input where tokens are masked.  \r\n",
    "        - Fill in the blanks for sentces.  \r\n",
    "    2. **Next Sentencediction (NSP)**  \r\n",
    "        - Provide two sentences A and B.  \r\n",
    "        - Predict if B wld follow A.  \r\n",
    "- **tiple mel ses**:\r\n",
    "    - **Base** – 100M parameters (12 layers, 12 attention heads, hidden se of  ).  \r\n",
    "    - **Large** – 240M parameters (24 layers, 16 attention heads, hidden se of024).  \r\n",
    "    - **Tiny** – 4M parameters (2 layers, 2 attention heads, hien size of 128).  \r\n",
    "*Fine-tune to perform**:\r\n",
    "    1. Named Entity Recognition (NER).  \r\n",
    "    2. Question Answering (QA).  \r\n",
    "    3. Sentence Pair Tasks.  \r\n",
    "    4. Summarization.  \r\n",
    "    5. Feature Extraction / Embeddings.  \r\n",
    "    6. Classification of text.  \r\n",
    "  7. Sentimennalysis.  \r\n",
    "- **BERT Variants**:\r\n",
    "    - RoBERTa.  \r\n",
    "    - DistilBERT.  \r\n",
    "    - ALBERT.  \r\n",
    "    - ELECTRA.  \r\n",
    "    - DeBERTa.  \r\n",
    "- Very useful for a specific task or reaching a spe\n",
    "onclion from a sce of text.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "- **Transformers** come in two pieces - encoder and decoder (natural language into mathematical representation and vice versa).  \r\n",
    "- First-gen LLMs had an encoder-decoder couple.  \r\n",
    "- GPTs are decoder-only.  \r\n",
    "- Encoder-only models take in natural language and return mathematical representations.  \r\n",
    "- RNNs are designed to have sequential understanding and process sequential data.  \r\n",
    "- With the transformer architecture, the sequce changes andhe understanding changes. \r\n",
    "- **Emergent tasks** - Capabilities that arise spontaneously from the model’s scale and training data, enabling it to perform complex functions—such as reasoning, translation, or coding—without explicit programming or task-specific training.\r\n",
    "- The bigger the model se, the mo number of emergent tasks it has.\r\n",
    "- **Embedding** refers to the numerical representation ofnput text that ctures semantic meaning.\r\n",
    "    - **Token Embeddings** – Convert each word or suord (WordPiece toke into a dense vector.\r\n",
    "    - **Position Embeddings** – Encode the position of each ken in the sequenc o maintain word order.\r\n",
    "    - **Segment Embeddings** – Distinguish between sentences in tasks like next-sentence prediction.\r\n",
    "- [Chinchilla Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)\r\n",
    "    - The bigger the model, the more information it can understand.\r\n",
    "    - The bigger the model, the more data it needs.\r\n",
    "    - Research outcome - Many models are overparameterized and undertrained (a lot of connections between nodes, but the amount of passes going through is too few).\r\n",
    "    - The dataset and token should be 20 times the number of parameters.\r\n",
    "- The models have the same size, but the parameter size differs based on the weights assigned.\r\n",
    "- The file size changes due to thsystem attachinmby adata o or the model training states.\r\n",
    "- **Optimize models** - pruning, teacher-student model (knowledge distillation)..n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c998fced-41ca-4bf9-bf6f-d893022bdb9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e86e8c3-1f21-4b4e-87a9-569cc942d29a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c8149-a321-41af-93fc-66e6756b1761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
